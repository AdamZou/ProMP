import tensorflow as tf
from collections import OrderedDict
from maml_zoo.utils.utils import get_original_tf_name

class Policy(object):
    """
    A container for storing the current pre and post update policies
    Also provides functions for executing and updating policy parameters
    Note: the preupdate policy is stored as tf.Variables, while the postupdate
    policy is stored in numpy arrays and executed through tf.placeholders
    Args:

    """
    def __init__(self,
                 name='policy',
                 hidden_sizes=(32, 32),
                 learn_std=True,
                 hidden_nonlinearity='tanh',
                 output_nonlinearity=None,
                 **kwargs
                 ):
        self.param_assign_ops = None
        self.param_assign_placeholders = None

    def build_graph(self, env_spec, **kwargs):
        """
        Also should create lists of variables and corresponding assign ops
        """
        # Why take kwargs?
        raise NotImplementedError

    def get_action(self, observation):
        """
        Runs a single observation through the specified policy
        Args:
            observation (array) : single observation
        Returns:
            (array) : array of arrays of actions for each env
        """
        raise NotImplementedError

    def get_actions(self, observations):
        """
        Runs each set of observations through each task specific policy
        Args:
            observations (array) : array of arrays of observations generated by each task and env
        Returns:
            (tuple) : array of arrays of actions for each env (meta_batch_size) x (batch_size) x (action_dim)
                      and array of arrays of agent_info dicts 
        """
        raise NotImplementedError

    def reset(self, dones=None):
        pass

    def log_diagnostics(self, paths):
        """
        Log extra information per iteration based on the collected paths
        """
        pass

    @property
    def distribution(self):
        """
        Returns:
            (Distribution) : this policy's distribution
        """
        raise NotImplementedError

    def distribution_info_sym(self, obs_var, params=None):
        """
        Return the symbolic distribution information about the actions.
        Args:
            obs_var (placeholder) : symbolic variable for observations
            parmas (None or dict) : a dictionary of placeholders that contains information about the
            state of the policy at the time it received the observation
        Returns:
            (dict) : a dictionary of tf placeholders for the policy output distribution
        """
        raise NotImplementedError

    def distribution_info_keys(self, obs, state_infos):
        """
        Args:
            obs (placeholder) : symbolic variable for observations
            state_infos (dict) : a dictionary of placeholders that contains information about the
            state of the policy at the time it received the observation
        Returns:
            (dict) : a dictionary of tf placeholders for the policy output distribution
        """
        raise NotImplementedError

    def get_params(self):
        """
        Get the tf.Variables representing the trainable weights of the network (symbolic)
        Returns: 
            (list) : list of all trainable Variables
        """
        return self.params

    def get_param_values(self):
        """
        Gets a list of all the current weights in the network (in original code it is flattened, why?)
        Returns:
            (list) : list of values for parameters
        """
        param_values = tf.get_default_session().run(self.params)
        return param_values

    def set_params(self, policy_params):
        """
        Sets the parameters for the graph
        Args:
            policy_params (array): array of policy parameters for each task
        """
        tf.get_default_session().run(self.param_assign_ops,
                                     feed_dict=dict(list(zip(self.param_assign_placeholders, policy_params))))

    def _create_getter_setter(self):
        """
        Creates the variables necessary for get_params and set_params. Call once during graph creation
        """
        self.params = tf.trainable_variables(scope=self.name)
        param_values = tf.get_default_session().run(self.params)
        self.param_assign_placeholders = []
        self.param_assign_ops = []
        for param in self.params:
            assign_placeholder = tf.placeholder(dtype=param.dtype.base_dtype)
            assign_op = tf.assign(param, assign_placeholder)
            self.param_assign_placeholders.append(assign_placeholder)
            self.param_assign_ops.append(assign_op)


class MetaPolicy(Policy):

    def __init__(self, *args, **kwargs):
        super(MetaPolicy, self).__init__(*args, **kwargs)
        self._pre_update_mode = True
        self.policies_params_vals = None

    def build_graph(self, env_spec, num_tasks=1):
        """
        Also should create lists of variables and corresponding assign ops
        """
        raise NotImplementedError

    def _create_placeholders_for_vars(self, scopes, num_tasks=1, graph_keys=tf.GraphKeys.TRAINABLE_VARIABLES):
        assert isinstance(scopes, list) or isinstance(scopes, tuple)
        placeholders = []

        for scope in scopes:
            var_list = tf.get_collection(graph_keys, scope=scope)
            placeholders.append([OrderedDict([(get_original_tf_name(var.name),
                                              tf.placeholder(tf.float32, shape=var.shape))
                                             for var in var_list])
                                for _ in range(num_tasks)
                                 ])
        return placeholders

    def switch_to_pre_update(self):
        """
        It switches to the pre-updated policy
        """
        self._pre_update_mode = True
        self.policies_params_vals = None

    def get_actions(self, observations):
        if self._pre_update_mode:
            return self._get_pre_update_actions(observations)
        else:
            return self._get_post_update_actions(observations)

    def _get_pre_update_actions(self, observations):
        """
        Args:
            observations (list): List of size meta-batch size with numpy arrays of shape batch_size x obs_dim

        """
        raise NotImplementedError

    def _get_post_update_actions(self, observations):
        """
        Args:
            observations (list): List of size meta-batch size with numpy arrays of shape batch_size x obs_dim

        """
        raise NotImplementedError

    def update_task_parameters(self, updated_policies_parameters):
        """
        Args:
            updated_policies_parameters (list): List of size meta-batch size. Each contains a dict with the policies
            parameters

        """
        self.policies_params_vals = updated_policies_parameters
        self._pre_update_mode = False
