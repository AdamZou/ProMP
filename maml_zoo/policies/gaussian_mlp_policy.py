from maml_zoo.policies.networks.mlp import create_mlp, forward_mlp
import tensorflow as tf
import numpy as np
from maml_zoo.policies.distributions.diagonal_gaussian import DiagonalGaussian
from maml_zoo.policies.base import Policy
from collections import OrderedDict
from maml_zoo.utils.utils import get_original_tf_name


class GaussianMLPPolicy(Policy):
    def __init__(self,
                 name='gaussian_mlp_policy',
                 hidden_sizes=(32, 32),
                 learn_std=True,
                 hidden_nonlinearity=tf.tanh,
                 output_nonlinearity=None,
                 init_std=1,
                 min_std=1e-6,
                 ):
        """
        Also provides functions for executing and updating policy parameters
        A container for storing the current pre and post update policies
        Args:

        """
        self.name = name
        self.hidden_sizes = hidden_sizes
        self.learn_std = learn_std
        self.hidden_nonlinearity = hidden_nonlinearity
        self.output_nonlinearity = output_nonlinearity
        self.min_log_std = np.log(min_std)
        self.init_log_std = np.log(init_std)

        self._env_spec = None
        self.init_policy = None
        self.policy_params = None
        self.obs_var = None
        self.mean_var = None
        self.log_std_var = None
        self.action_var = None
        self._dist = None
        self.obs_dim = None
        self.action_dim = None

    def build_graph(self, env_spec, **kwargs):
        self.obs_dim = env_spec.observation_space.flat_dim
        self.action_dim = env_spec.action_space.flat_dim

        with tf.variable_scope(self.name):
            obs_var, mean_var = create_mlp(name='mean_network',
                                           output_dim=self.action_dim,
                                           hidden_sizes=self.hidden_sizes,
                                           hidden_nonlinearity=self.hidden_nonlinearity,
                                           output_nonlinearity=self.output_nonlinearity,
                                           input_dim=(None, self.obs_dim)
                                           )

            log_std_var = tf.get_variable(name='log_std_network',
                                          shape=(1, self.action_dim,),
                                          dtype=tf.float32,
                                          initializer=tf.constant_initializer(self.init_log_std),
                                          trainable=self.learn_std
                                          )

            log_std_var = tf.maximum(log_std_var, self.min_log_std)

            current_scope = tf.get_default_graph().get_name_scope()
            mean_network_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                                  scope=current_scope + '/mean_network')
            log_std_network_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES,
                                                     scope=current_scope + '/log_std_network')

            assert len(log_std_network_vars) == 1

            mean_network_vars = OrderedDict([(get_original_tf_name(var.name), var) for var in mean_network_vars])
            log_std_network_vars = OrderedDict([(get_original_tf_name(var.name), var) for var in log_std_network_vars])

            self._create_getter_setter()

        action_var = mean_var + tf.random_normal(shape=mean_var.shape) * tf.exp(log_std_var)

        self.obs_var = obs_var
        self.action_var = action_var
        self.mean_var = mean_var
        self.log_std_var = log_std_var
        self.policy_params = mean_network_vars.update(log_std_network_vars)

        self._dist = DiagonalGaussian(self.action_dim)

    def get_action(self, observation):
        """
        Runs a single observation through the specified policy
        Args:
            observation (ndarray) : single observation
            policy_params (params) :
        Returns:
            (ndarray) : array of arrays of actions for each env
        """
        observation = np.expand_dims(observation, axis=0)
        action, agent_infos = self.get_actions(observation)
        action, agent_infos = action[0], dict(mean=agent_infos['mean'][0], log_std=agent_infos['log_std'][0])
        return action, agent_infos

    def get_actions(self, observations):
        """
        Runs each set of observations through each task specific policy
        Args:
            observations (ndarray) : array of arrays of observations generated by each task and env
        Returns:
            (ndarray) : array of arrays of actions for each env
        """
        sess = tf.get_default_session()
        actions, means, logs_stds = sess.run([self.action_var, self.mean_var, self.log_std_var],
                                             feed_dict={self.obs_var: observations})
        rnd = np.random.normal(size=means.shape)
        actions = means + rnd * np.exp(logs_stds)
        return actions, dict(mean=means, log_std=logs_stds)

    def log_diagnostics(self, paths):
        """
        Log extra information per iteration based on the collected paths
        """
        raise NotImplementedError

    def load_params(self, policy_params):
        """
        Args:
            policy_params (ndarray): array of policy parameters for each task
        """
        raise NotImplementedError

    @property
    def distribution(self):
        """
        Returns:
            (Distribution) : this policy's distribution
        """
        return self._dist

    def output_sym(self, obs_var, params=None):
        """
        Return the symbolic distribution information about the actions.
        Args:
            obs_var (placeholder) : symbolic variable for observations
            params (dict) : a dictionary of placeholders or vars with the parameters of the MLP
        Returns:
            (dict) : a dictionary of tf placeholders for the policy output distribution
        """
        with tf.variable_scope(self.name):
            if params is None:
                obs_var, mean_var = create_mlp(name='mean_network',
                                               output_dim=self.action_dim,
                                               hidden_sizes=self.hidden_sizes,
                                               hidden_nonlinearity=self.hidden_nonlinearity,
                                               output_nonlinearity=self.output_nonlinearity,
                                               input_var=obs_var,
                                               reuse=True,
                                               )

                log_std_var = self.log_std_var
            else:
                mean_network_params = []
                log_std_network_params = []
                for param in params.values():
                    if 'mean_network' in param.name:
                        mean_network_params.append(param)
                    elif 'log_std_network' in param.name:
                        log_std_network_params.append(params)

                assert len(log_std_network_params) == 1

                obs_var, mean_var = forward_mlp(output_dim=self.obs_dim,
                                                hidden_sizes=self.hidden_sizes,
                                                hidden_nonlinearity=self.hidden_nonlinearity,
                                                output_nonlinearity=self.output_nonlinearity,
                                                input_var=obs_var,
                                                mlp_params=mean_network_params,
                                                )

                log_std_var = log_std_network_params[0]

        return dict(mean=mean_var, log_std=log_std_var)

    def output_info(self, obs, state_infos):
        """
        Args:
            obs (placeholder) : symbolic variable for observations
            state_infos (dict) : a dictionary of placeholders that contains information about the
            state of the policy at the time it received the observation
        Returns:
            (dict) : a dictionary of tf placeholders for the policy output distribution
        """
        raise NotImplementedError


