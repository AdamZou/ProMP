import tensorflow as tfimport numpy as npdef log_prob_gaussian(x_var, std):    return -1 / (2 * std) * (x_var) ** 2 - 1 / 2 * tf.log(2 * np.pi * std ** 2)def get_phs(horizon, sufix=''):    act = tf.placeholder(dtype=tf.float32, shape=(None, horizon), name='act_'+sufix)    obs = tf.placeholder(dtype=tf.float32, shape=(None, horizon), name='obs_'+sufix)    adv = tf.placeholder(dtype=tf.float32, shape=(None, horizon), name='adv_'+sufix)    return act, obs, advclass FeaturePolicy(object):    def __init__(self, feature, std=0.1):        self.feature = feature        self.params = params = tf.get_variable("theta", shape=(2,), dtype=tf.float32,                                               initializer=tf.ones_initializer)        self.obs_var = obs_var = tf.placeholder(dtype=tf.float32, shape=(None,), name='input')        self.mean_var = self.get_mean(obs_var)        self.std = std    def get_mean(self, obs_var, params=None):        if params is None:            return self.feature(self.params[0]) * obs_var + self.params[1]        else:            return self.feature(params[0]) * obs_var + params[1]    def get_actions(self, observations):        sess = tf.get_default_session()        means = sess.run(self.mean_var, feed_dict={self.obs_var:observations})        rnd = np.random.normal(size=len(observations))        actions = means + rnd * self.std        return actions    def log_prob_sym(self, act, obs_var, params=None):        return -1/ (2 * self.std) * (act - self.get_mean(obs_var, params)) ** 2 - 1/2 * tf.log(2 * np.pi * self.std ** 2)    def get_params(self):        sess = tf.get_default_session()        return sess.run(self.params)class Algo(object):    def __init__(self, policy, horizon, num_traj, type, init_state_std=1, inner_lr=0.1, lr=0.1):        """        :param policy:        :param horizon:        :param num_traj: Refers to the number of trajectories per meta_task        :param type:        :param init_state_std:        :param inner_lr:        :param lr:        """        assert type in ['dice', 'exp', 'robust']        self.policy = policy        self.horizon = horizon        self.num_traj = num_traj        self.type = type        self.init_state_std = init_state_std        self.inner_lr = inner_lr        self._lr = lr        self.meta_tasks = 2        self._build()    def _build(self):        loss_list = []        inner_grads_list = []        outer_grads_list = []        meta_grads_list = []        inputs_list_0 = []        inputs_list_1 = []        for j in range(self.num_traj):            for i in range(self.meta_tasks):                act_1, obs_1, adv_1 = get_phs(self.horizon, 'step1_traj%d_task%d' % (j, i))                inputs_list_1.extend([act_1, obs_1, adv_1])                if self.type == 'robust':                    adapted_params = None                    inner_grads = None                else:                    act_0, obs_0, adv_0 = get_phs(self.horizon, 'step0_traj%d_task%d' % (j, i))                    inputs_list_0.extend([act_0, obs_0, adv_0])                    inner_loss = self.inner_loss(act_0, obs_0, adv_0)                    inner_grads = self.get_grads(inner_loss, self.policy.params)                    adapted_params = self.policy.params - self.inner_lr * inner_grads                prob_traj_1 = self.log_prob_traj(act_1, obs_1, adapted_params)                if self.type == 'exp':                    prob_traj_0 = self.log_prob_traj(act_0, obs_0)                    loss = - tf.reduce_mean(                        self.policy.log_prob_sym(act_1, obs_1, adapted_params) * adv_1 * prob_traj_1 * prob_traj_0                    )                else:                    loss = - tf.reduce_mean(self.policy.log_prob_sym(act_1, obs_1, adapted_params) * adv_1)                if self.type == 'robust':                    outer_grads = None                else:                    outer_grads = tf.gradients(loss, adapted_params)                meta_grads = self.get_grads(loss, self.policy.params)                inner_grads_list.append(inner_grads)                outer_grads_list.append(outer_grads)                meta_grads_list.append(meta_grads)                loss_list.append(loss)            self.loss = tf.reduce_mean(loss_list)            self._inner_grads = inner_grads_list            self._outer_grads = outer_grads_list            self._meta_grads = meta_grads_list            self.grad_step = tf.train.GradientDescentOptimizer(self._lr).minimize(self.loss)            self._inputs_0 = inputs_list_0            self._inputs_1 = inputs_list_1    # def get_grads_prob(self, act_var, obs_var, params=None):    #     log_prob = self.log_prob_traj(act_var, obs_var, params)    #     grads = self.get_grads(log_prob, params)    def get_grads(self, loss, variables):        grads = tf.gradients(loss, variables)        return grads    def inner_loss(self, act_var, obs_var, adv_var):        if self.type == 'dice':            prob_traj = self.log_prob_traj(act_var, obs_var)            loss = - tf.reduce_mean(self.policy.log_prob_sym(act_var, obs_var) * adv_var * tf.exp(prob_traj))        elif self.type == 'exp':            loss = - tf.reduce_mean(self.policy.log_prob_sym(act_var, obs_var) * adv_var, ) # TODO: Not sure if there is actually a reduce_sum        else:            loss = None        return loss    def log_prob_traj(self, act_var, obs_var, params=None):        """        This assumes that the obs_var has size (Samples, traj_length)        :param act_var:        :param obs_var:        :return:        """        init_log_prob = log_prob_gaussian(obs_var[:,0], self.init_state_std)        log_probs = self.policy.log_prob_sym(act_var, obs_var, params=params)        log_prob_traj = tf.expand_dims(tf.reduce_sum(log_probs, axis=-1) + init_log_prob, axis=-1)        return log_prob_traj    def adapt(self, data):        raise NotImplementedError    def train(self, data):        if self.type == 'robust':            processed_data = sum([[np.stack(v['act'], axis=-1),                                   np.stack(v['obs'], axis=-1),                                   np.stack(v['adv'], axis=-1)] for v in data.values()], [])            sess = tf.get_default_session()            feed_dict = dict(zip(self._inputs_1, processed_data))            sess.run(self.grad_step, feed_dict=feed_dict)        else:            raise NotImplementedErrordef collect_samples(policy, num_samples, traj_len, init_state_std=1,): # TODO: I need to pass the goal here for the meta-learning adaptation!    goal = [-1. ,1]    data = {-1: dict(act=list(), obs=list(), adv=list()), 1: dict(act=list(), obs=list(), adv=list())}    for g in goal:        s = np.random.normal(scale=init_state_std, size=(num_samples,))        for t in range(traj_len):            a = policy.get_actions(s)            r = (a - g) ** 2            data[g]['obs'].append(s)            data[g]['act'].append(a)            data[g]['adv'].append(r)            s = a.copy()            # TODO: Do a cum sum!!    return dataif __name__ == "__main__":    feature = lambda x: x    policy = FeaturePolicy(feature)    horizon = 1    num_samples = 10000    inner_lr = 0.1    lr = .01    opt_type = 'robust'    algo = Algo(policy, horizon, opt_type, init_state_std=1, inner_lr=inner_lr, lr=lr)    with tf.Session() as sess:        sess.run(tf.global_variables_initializer())        for _ in range(10000):            if opt_type == 'robust':                data = collect_samples(policy, num_samples, horizon)                algo.train(data)            print(policy.get_params())        print(data[1]['obs'][-1])        print("here")